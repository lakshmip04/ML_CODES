{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgkaTZlq53LZ"
      },
      "outputs": [],
      "source": [
        "#A1. Write your own functions for the following modules:\n",
        "# a) Summation unit\n",
        "# b) Activation Unit – Step, Bipolar Step, Sigmoid, TanH, ReLU and Leaky ReLU functions\n",
        "# c) Comparator unit for Error calculation\n",
        "\n",
        "import math\n",
        "\n",
        "def summation_unit(inputs, weights):\n",
        "    return sum(i * w for i, w in zip(inputs, weights))\n",
        "\n",
        "def step_function(x):\n",
        "    return 1 if x >= 0 else 0\n",
        "\n",
        "def bipolar_step_function(x):\n",
        "    return 1 if x >= 0 else -1\n",
        "\n",
        "def sigmoid_function(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "def tanh_function(x):\n",
        "    return math.tanh(x)\n",
        "\n",
        "def relu_function(x):\n",
        "    return max(0, x)\n",
        "\n",
        "def leaky_relu_function(x):\n",
        "    return x if x >= 0 else 0.01 * x\n",
        "\n",
        "def comparator_unit(predicted, actual):\n",
        "    return actual - predicted\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A2. Develop the above perceptron in your own code (don’t use the perceptron model available from\n",
        "# package). Use the initial weights as provided below.\n",
        "\n",
        "def train_perceptron_and_gate(inputs, outputs, epochs=1000, lr=0.05):\n",
        "    weights = [10, 0.2, -0.75]\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(inputs)):\n",
        "            summation = summation_unit([1] + inputs[i], weights)\n",
        "            prediction = step_function(summation)\n",
        "            error = comparator_unit(prediction, outputs[i])\n",
        "            total_error += error ** 2\n",
        "            for j in range(len(weights)):\n",
        "                weights[j] += lr * error * ([1] + inputs[i])[j]\n",
        "        if total_error <= 0.002:\n",
        "            break\n",
        "    return weights, epoch\n",
        "\n",
        "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "outputs = [0, 0, 0, 1]\n",
        "weights, epochs = train_perceptron_and_gate(inputs, outputs)\n",
        "print(\"Weights:\", weights)\n",
        "print(\"Epochs:\", epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ0j6uV56yCQ",
        "outputId": "7d69124e-437f-4b58-e2ed-f8f2b8ba9ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [-0.10000000000000765, 0.1000000000000001, 0.05000000000000032]\n",
            "Epochs: 129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A3. Repeat the above A1 experiment with following activation functions\n",
        "\n",
        "def train_with_activation(inputs, outputs, activation_func, epochs=1000, lr=0.05):\n",
        "    weights = [10, 0.2, -0.75]\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(inputs)):\n",
        "            summation = summation_unit([1] + inputs[i], weights)\n",
        "            prediction = activation_func(summation)\n",
        "            error = comparator_unit(prediction, outputs[i])\n",
        "            total_error += error ** 2\n",
        "            for j in range(len(weights)):\n",
        "                weights[j] += lr * error * ([1] + inputs[i])[j]\n",
        "        if total_error <= 0.002:\n",
        "            break\n",
        "    return weights, epoch\n",
        "\n",
        "# Example using Sigmoid Function\n",
        "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]] # Define inputs and outputs here\n",
        "outputs = [0, 0, 0, 1]\n",
        "weights, epochs = train_with_activation(inputs, outputs, sigmoid_function)\n",
        "\n",
        "print(\"Weights:\", weights)\n",
        "print(\"Epochs:\", epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpL0F6ai64v3",
        "outputId": "7aa786e5-09ff-42db-eb02-53a5cdee0167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [-6.1353130787344305, 3.970833206406072, 3.963948347259991]\n",
            "Epochs: 999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A4. Repeat exercise A1 with varying the learning rate, keeping the initial weights same.\n",
        "\n",
        "def train_perceptron_and_gate(inputs, outputs, epochs=1000, lr=0.05): # Added the function definition back\n",
        "    weights = [10, 0.2, -0.75]\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(inputs)):\n",
        "            summation = summation_unit([1] + inputs[i], weights)\n",
        "            prediction = step_function(summation)\n",
        "            error = comparator_unit(prediction, outputs[i])\n",
        "            total_error += error ** 2\n",
        "            for j in range(len(weights)):\n",
        "                weights[j] += lr * error * ([1] + inputs[i])[j]\n",
        "        if total_error <= 0.002:\n",
        "            break\n",
        "    return weights, epoch\n",
        "\n",
        "learning_rates = [0.1 * i for i in range(1, 11)]\n",
        "iterations = []\n",
        "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]  # Define inputs and outputs here\n",
        "outputs = [0, 0, 0, 1]\n",
        "\n",
        "for lr in learning_rates:\n",
        "    _, epoch = train_perceptron_and_gate(inputs, outputs, lr=lr)\n",
        "    iterations.append(epoch)\n",
        "\n",
        "    print(f\"Learning Rate: {lr}, Epochs: {epoch}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r4xCtsH7ADJ",
        "outputId": "f69dfc85-7f81-4711-fc5b-bee72400640e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate: 0.1, Epochs: 67\n",
            "Learning Rate: 0.2, Epochs: 36\n",
            "Learning Rate: 0.30000000000000004, Epochs: 22\n",
            "Learning Rate: 0.4, Epochs: 22\n",
            "Learning Rate: 0.5, Epochs: 18\n",
            "Learning Rate: 0.6000000000000001, Epochs: 18\n",
            "Learning Rate: 0.7000000000000001, Epochs: 14\n",
            "Learning Rate: 0.8, Epochs: 13\n",
            "Learning Rate: 0.9, Epochs: 12\n",
            "Learning Rate: 1.0, Epochs: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A5. Repeat the above exercises, A1 to A3, for XOR gate logic.\n",
        "\n",
        "inputs_xor = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "outputs_xor = [0, 1, 1, 0]\n",
        "weights, epochs = train_perceptron_and_gate(inputs_xor, outputs_xor)\n",
        "print(\"Weights:\", weights)\n",
        "print(\"Epochs:\", epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQiehSes7Lmh",
        "outputId": "47512fc5-1754-46bf-d739-21744fb7064a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [0.09999999999999236, -0.09999999999999969, -0.09999999999999969]\n",
            "Epochs: 999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A6. Use customer data provided below.\n",
        "\n",
        "def train_perceptron_customers(data, labels, epochs=1000, lr=0.05):\n",
        "    weights = [0.1, 0.2, 0.3, 0.4]  # Example initial weights\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(data)):\n",
        "            summation = summation_unit([1] + data[i], weights)\n",
        "            prediction = sigmoid_function(summation)\n",
        "            error = comparator_unit(prediction, labels[i])\n",
        "            total_error += error ** 2\n",
        "            for j in range(len(weights)):\n",
        "                weights[j] += lr * error * ([1] + data[i])[j]\n",
        "        if total_error <= 0.002:\n",
        "            break\n",
        "    return weights, epoch\n",
        "\n",
        "customer_data = [\n",
        "    [20, 6, 2],\n",
        "    [16, 3, 6],\n",
        "    [27, 6, 2],\n",
        "    [19, 1, 2],\n",
        "    [24, 4, 2],\n",
        "    [22, 1, 5],\n",
        "    [15, 4, 2],\n",
        "    [18, 4, 2],\n",
        "    [21, 1, 4],\n",
        "    [16, 2, 4]\n",
        "]\n",
        "\n",
        "high_value_labels = [1, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
        "\n",
        "weights, epochs = train_perceptron_customers(customer_data, high_value_labels)\n",
        "\n",
        "print(\"Weights:\", weights)\n",
        "print(\"Epochs:\", epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKv9vDn87ShP",
        "outputId": "ea1484ed-935c-44c3-f80f-472dcd327846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [-0.18250271389279019, -1.3365214891771673, 9.012435167930922, -0.0533006109983556]\n",
            "Epochs: 122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A7. Compare the results obtained from above perceptron learning to the ones obtained with matrix\n",
        "# pseudo-inverse\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def pseudo_inverse_solution(data, labels):\n",
        "    X = np.array([[1] + d for d in data])\n",
        "    Y = np.array(labels)\n",
        "    pseudo_inv = np.linalg.pinv(X)\n",
        "    weights = np.dot(pseudo_inv, Y)\n",
        "    return weights\n",
        "\n",
        "weights_pseudo = pseudo_inverse_solution(customer_data, high_value_labels)\n",
        "print(\"Pseudo-Inverse Weights:\", weights_pseudo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-mpUPD07TwZ",
        "outputId": "84526f89-c511-47f2-e0c3-e8a58f95cc72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pseudo-Inverse Weights: [ 0.1139903  -0.02342675  0.2607237   0.03727212]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A8. Develop the below Neural Network. Use learning rate (α) = 0.05 with a Sigmoid activation function.\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def train_nn_and_gate(inputs, outputs, epochs=1000, lr=0.05):\n",
        "    weights_input_hidden = [0.5, -0.6, 0.2]\n",
        "    weights_hidden_output = [0.4, -0.7]\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(inputs)):\n",
        "            hidden_input = summation_unit([1] + inputs[i], weights_input_hidden)\n",
        "            hidden_output = sigmoid_function(hidden_input)\n",
        "            output_input = summation_unit([1, hidden_output], weights_hidden_output)\n",
        "            output = sigmoid_function(output_input)\n",
        "            error = comparator_unit(output, outputs[i])\n",
        "            total_error += error ** 2\n",
        "            delta_output = error * sigmoid_derivative(output)\n",
        "            delta_hidden = delta_output * sigmoid_derivative(hidden_output) * weights_hidden_output[1]\n",
        "            weights_hidden_output[0] += lr * delta_output * 1\n",
        "            weights_hidden_output[1] += lr * delta_output * hidden_output\n",
        "            for j in range(len(weights_input_hidden)):\n",
        "                weights_input_hidden[j] += lr * delta_hidden * ([1] + inputs[i])[j]\n",
        "        if total_error <= 0.002:\n",
        "            break\n",
        "    return weights_input_hidden, weights_hidden_output, epoch\n",
        "\n",
        "weights_input_hidden, weights_hidden_output, epochs = train_nn_and_gate(inputs, outputs)\n",
        "print(\"Weights Input to Hidden:\", weights_input_hidden)\n",
        "print(\"Weights Hidden to Output:\", weights_hidden_output)\n",
        "print(\"Epochs:\", epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouQjJbuz7Ycf",
        "outputId": "86bc69ec-0c1f-4937-d6b3-7f30aaa5c322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights Input to Hidden: [1.065546038523148, -1.6039201646705568, -1.103603376806378]\n",
            "Weights Hidden to Output: [0.05922996021700995, -2.45894383324324]\n",
            "Epochs: 999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A9. Repeat the above A1 experiment for XOR Gate logic. Keep the learning rate & activation function\n",
        "# same as A1\n",
        "\n",
        "weights_input_hidden, weights_hidden_output, epochs = train_nn_and_gate(inputs_xor, outputs_xor)\n",
        "print(\"Weights Input to Hidden:\", weights_input_hidden)\n",
        "print(\"Weights Hidden to Output:\", weights_hidden_output)\n",
        "print(\"Epochs:\", epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuHEAY0L7bI_",
        "outputId": "72167925-c6c3-4691-90b6-dabb4679d5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights Input to Hidden: [0.5616681385599502, -0.5607471249996671, 0.3093566144322304]\n",
            "Weights Hidden to Output: [0.40664727753188135, -0.6764470646287483]\n",
            "Epochs: 999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A10. Repeat exercise A1 & A2 with 2 output nodes (as shown below).\n",
        "\n",
        "def train_perceptron_2_outputs(inputs, outputs, epochs=1000, lr=0.05):\n",
        "    weights = [[0.1, 0.2, -0.1], [0.05, 0.3, -0.25]]\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for i in range(len(inputs)):\n",
        "            summation_1 = summation_unit([1] + inputs[i], weights[0])\n",
        "            summation_2 = summation_unit([1] + inputs[i], weights[1])\n",
        "            prediction_1 = step_function(summation_1)\n",
        "            prediction_2 = step_function(summation_2)\n",
        "            error_1 = comparator_unit(prediction_1, outputs[i][0])\n",
        "            error_2 = comparator_unit(prediction_2, outputs[i][1])\n",
        "            total_error += error_1 ** 2 + error_2 ** 2\n",
        "            for j in range(len(weights[0])):\n",
        "                weights[0][j] += lr * error_1 * ([1] + inputs[i])[j]\n",
        "                weights[1][j] += lr * error_2 * ([1] + inputs[i])[j]\n",
        "        if total_error <= 0.002:\n",
        "            break\n",
        "    return weights, epoch\n",
        "\n",
        "outputs_2 = [[1, 0], [1, 0], [1, 0], [0, 1]]  # Example for AND Gate logic\n",
        "weights, epochs = train_perceptron_2_outputs(inputs, outputs_2)\n",
        "print(\"Weights:\", weights)\n",
        "print(\"Epochs:\", epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2A879hH7eVn",
        "outputId": "dd75eade-3142-457c-e2e8-0d41ead2e43f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [[0.10000000000000002, -0.09999999999999999, -0.10000000000000002], [-0.2, 0.15000000000000002, 0.04999999999999999]]\n",
            "Epochs: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A11. Learn using a MLP network from Sci-Kit manual available.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# AND Gate using MLPClassifier\n",
        "mlp_and = MLPClassifier(hidden_layer_sizes=(), activation='logistic', solver='sgd', max_iter=1000)\n",
        "mlp_and.fit(inputs, outputs)\n",
        "and_predictions = mlp_and.predict(inputs)\n",
        "\n",
        "# XOR Gate using MLPClassifier\n",
        "mlp_xor = MLPClassifier(hidden_layer_sizes=(), activation='logistic', solver='sgd', max_iter=1000)\n",
        "mlp_xor.fit(inputs_xor, outputs_xor)\n",
        "xor_predictions = mlp_xor.predict(inputs_xor)\n",
        "\n",
        "# Print the predictions\n",
        "print(\"AND Gate Predictions:\")\n",
        "print(and_predictions)\n",
        "\n",
        "print(\"\\nXOR Gate Predictions:\")\n",
        "print(xor_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEQ_VmO67opS",
        "outputId": "8b899061-4c24-49b9-e37b-b488c614eeb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate Predictions:\n",
            "[0 0 0 0]\n",
            "\n",
            "XOR Gate Predictions:\n",
            "[0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# customer data\n",
        "mlp_project = MLPClassifier(hidden_layer_sizes=(5,), activation='logistic', solver='sgd', max_iter=1000)\n",
        "mlp_project.fit(customer_data, high_value_labels)\n",
        "project_predictions = mlp_project.predict(customer_data)\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Customer Data Predictions:\")\n",
        "print(project_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icttZPZT7vDp",
        "outputId": "85154002-b02f-4732-cd1d-218edeb91201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer Data Predictions:\n",
            "[1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A12. Use the MLPClassifier() function on your project dataset.\n",
        "\n",
        "def load_project_data(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    X = df.iloc[:, :-1].values  # Assuming the last column is the target variable\n",
        "    y = df.iloc[:, -1].values   # Target column (assumed to be the last one)\n",
        "    return X, y\n",
        "\n",
        "# AND Gate Example (A2)\n",
        "print(\"Training Perceptron for AND Gate Logic...\")\n",
        "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "outputs = [0, 0, 0, 1]\n",
        "weights, epochs = train_perceptron_and_gate(inputs, outputs)\n",
        "print(f\"Weights after training: {weights}\")\n",
        "print(f\"Epochs to converge: {epochs}\\n\")\n",
        "# XOR Gate Example (A5)\n",
        "print(\"Training Perceptron for XOR Gate Logic...\")\n",
        "inputs_xor = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "outputs_xor = [0, 1, 1, 0]\n",
        "weights_xor, epochs_xor = train_perceptron_and_gate(inputs_xor, outputs_xor)\n",
        "print(f\"Weights after training for XOR: {weights_xor}\")\n",
        "print(f\"Epochs to converge: {epochs_xor}\\n\")\n",
        "# Customer Data Example (A6)\n",
        "print(\"Training Perceptron for Customer Data...\")\n",
        "customer_data = [\n",
        "    [20, 6, 2],\n",
        "    [16, 3, 6],\n",
        "    [27, 6, 2],\n",
        "    [19, 1, 2],\n",
        "    [24, 4, 2],\n",
        "    [22, 1, 5],\n",
        "    [15, 4, 2],\n",
        "    [18, 4, 2],\n",
        "    [21, 1, 4],\n",
        "    [16, 2, 4]\n",
        "]\n",
        "high_value_labels = [1, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
        "weights_customers, epochs_customers = train_perceptron_customers(customer_data, high_value_labels)\n",
        "print(f\"Weights after training customer data: {weights_customers}\")\n",
        "print(f\"Epochs to converge: {epochs_customers}\\n\")\n",
        "# Comparing with Matrix Pseudo-Inverse (A7)\n",
        "print(\"Comparing Weights with Pseudo-Inverse Method...\")\n",
        "weights_pseudo = pseudo_inverse_solution(customer_data, high_value_labels)\n",
        "print(f\"Weights from pseudo-inverse: {weights_pseudo}\\n\")\n",
        "# Neural Network for AND Gate (A8)\n",
        "print(\"Training Neural Network for AND Gate...\")\n",
        "weights_input_hidden, weights_hidden_output, epochs_nn = train_nn_and_gate(inputs, outputs)\n",
        "print(f\"Weights (input-hidden) after training: {weights_input_hidden}\")\n",
        "print(f\"Weights (hidden-output) after training: {weights_hidden_output}\")\n",
        "print(f\"Epochs to converge: {epochs_nn}\\n\")\n",
        "# Neural Network for XOR Gate (A9)\n",
        "print(\"Training Neural Network for XOR Gate...\")\n",
        "weights_input_hidden_xor, weights_hidden_output_xor, epochs_nn_xor = train_nn_and_gate(inputs_xor, outputs_xor)\n",
        "print(f\"Weights (input-hidden) after training XOR: {weights_input_hidden_xor}\")\n",
        "print(f\"Weights (hidden-output) after training XOR: {weights_hidden_output_xor}\")\n",
        "print(f\"Epochs to converge: {epochs_nn_xor}\\n\")\n",
        "# Perceptron with 2 Output Nodes (A10)\n",
        "print(\"Training Perceptron with 2 Output Nodes for AND Gate...\")\n",
        "outputs_2 = [[1, 0], [1, 0], [1, 0], [0, 1]]\n",
        "weights_2_outputs, epochs_2_outputs = train_perceptron_2_outputs(inputs, outputs_2)\n",
        "print(f\"Weights for output 1 after training: {weights_2_outputs[0]}\")\n",
        "print(f\"Weights for output 2 after training: {weights_2_outputs[1]}\")\n",
        "print(f\"Epochs to converge: {epochs_2_outputs}\\n\")\n",
        "# Using MLPClassifier for AND Gate (A11)\n",
        "print(\"Using MLPClassifier for AND Gate...\")\n",
        "mlp_and = MLPClassifier(hidden_layer_sizes=(), activation='logistic', solver='sgd', max_iter=1000)\n",
        "mlp_and.fit(inputs, outputs)\n",
        "and_predictions = mlp_and.predict(inputs)\n",
        "print(f\"Predictions for AND Gate using MLPClassifier: {and_predictions}\\n\")\n",
        "# Using MLPClassifier for XOR Gate (A11)\n",
        "print(\"Using MLPClassifier for XOR Gate...\")\n",
        "mlp_xor = MLPClassifier(hidden_layer_sizes=(), activation='logistic', solver='sgd', max_iter=1000)\n",
        "mlp_xor.fit(inputs_xor, outputs_xor)\n",
        "xor_predictions = mlp_xor.predict(inputs_xor)\n",
        "print(f\"Predictions for XOR Gate using MLPClassifier: {xor_predictions}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLTHX66670-_",
        "outputId": "f33f84eb-a57e-40ba-c38e-00bb8c09a6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Perceptron for AND Gate Logic...\n",
            "Weights after training: [-0.10000000000000765, 0.1000000000000001, 0.05000000000000032]\n",
            "Epochs to converge: 129\n",
            "\n",
            "Training Perceptron for XOR Gate Logic...\n",
            "Weights after training for XOR: [0.09999999999999236, -0.09999999999999969, -0.09999999999999969]\n",
            "Epochs to converge: 999\n",
            "\n",
            "Training Perceptron for Customer Data...\n",
            "Weights after training customer data: [-0.18250271389279019, -1.3365214891771673, 9.012435167930922, -0.0533006109983556]\n",
            "Epochs to converge: 122\n",
            "\n",
            "Comparing Weights with Pseudo-Inverse Method...\n",
            "Weights from pseudo-inverse: [ 0.1139903  -0.02342675  0.2607237   0.03727212]\n",
            "\n",
            "Training Neural Network for AND Gate...\n",
            "Weights (input-hidden) after training: [1.065546038523148, -1.6039201646705568, -1.103603376806378]\n",
            "Weights (hidden-output) after training: [0.05922996021700995, -2.45894383324324]\n",
            "Epochs to converge: 999\n",
            "\n",
            "Training Neural Network for XOR Gate...\n",
            "Weights (input-hidden) after training XOR: [0.5616681385599502, -0.5607471249996671, 0.3093566144322304]\n",
            "Weights (hidden-output) after training XOR: [0.40664727753188135, -0.6764470646287483]\n",
            "Epochs to converge: 999\n",
            "\n",
            "Training Perceptron with 2 Output Nodes for AND Gate...\n",
            "Weights for output 1 after training: [0.10000000000000002, -0.09999999999999999, -0.10000000000000002]\n",
            "Weights for output 2 after training: [-0.2, 0.15000000000000002, 0.04999999999999999]\n",
            "Epochs to converge: 9\n",
            "\n",
            "Using MLPClassifier for AND Gate...\n",
            "Predictions for AND Gate using MLPClassifier: [0 0 0 0]\n",
            "\n",
            "Using MLPClassifier for XOR Gate...\n",
            "Predictions for XOR Gate using MLPClassifier: [0 1 0 0]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}